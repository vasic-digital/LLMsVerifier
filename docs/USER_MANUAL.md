# LLM Verifier - Complete User Manual

**Version**: 1.0.0  
**Last Updated**: December 25, 2025  

---

## üìö Table of Contents

1. [Getting Started](#getting-started)
2. [Installation](#installation)
3. [Quick Start](#quick-start)
4. [Configuration](#configuration)
5. [Verification](#verification)
6. [Monitoring](#monitoring)
7. [Brotli Compression](#brotli-compression)
8. [API Reference](#api-reference)
9. [Troubleshooting](#troubleshooting)
10. [Advanced Features](#advanced-features)

---

## üöÄ Getting Started

### System Requirements

- **Operating System**: Linux, macOS, Windows 10+
- **Go Version**: 1.21 or higher
- **Memory**: 2GB minimum, 4GB recommended
- **Disk Space**: 500MB free space
- **Network**: Internet connection for API verification

### Supported Providers

| Provider | Status | Features |
|----------|--------|----------|
| OpenAI | ‚úÖ Verified | Streaming, Function Calling, Vision, Brotli |
| Anthropic | ‚úÖ Verified | Streaming, Function Calling, Vision, Brotli |
| Google | ‚úÖ Verified | Streaming, Function Calling, Vision, Brotli |
| Meta | ‚úÖ Verified | Streaming, Brotli |
| Cohere | ‚úÖ Verified | Streaming, Brotli |
| Azure | ‚úÖ Verified | Streaming, Function Calling, Vision, Brotli |
| Amazon Bedrock | ‚úÖ Verified | Streaming, Function Calling, Brotli |

---

## üì¶ Installation

### Method 1: From Source

```bash
# Clone repository
git clone https://github.com/your-org/llm-verifier.git
cd llm-verifier

# Build binary
go build -o llm-verifier ./cmd

# Install globally
sudo mv llm-verifier /usr/local/bin/
```

### Method 2: Docker

```bash
# Pull latest image
docker pull llm-verifier:latest

# Run with default config
docker run -p 8080:8080 llm-verifier:latest

# Run with custom config
docker run -p 8080:8080 -v $(pwd)/config.yaml:/config llm-verifier:latest --config /config/config.yaml
```

### Method 3: Package Manager

```bash
# Go install (cross-platform)
go install github.com/your-org/llm-verifier@latest

# Run from anywhere
llm-verifier --help
```

---

## ‚ö° Quick Start

### 5-Minute Verification

```bash
# 1. Create configuration file
cat > config.yaml << EOF
llms:
  - name: "gpt-4"
    endpoint: "https://api.openai.com/v1"
    api_key: "your-api-key-here"
    features:
      - streaming
      - function-calling

# 2. Run verification
llm-verifier verify --config config.yaml
```

### Verify Specific Model

```bash
# Verify single model
llm-verifier verify \
  --provider openai \
  --model gpt-4 \
  --api-key sk-... \
  --check-brotli

# Output: Model exists, responsive, supports Brotli
```

---

## ‚öôÔ∏è Configuration

### Configuration File Structure

```yaml
# config.yaml
global:
  base_url: "https://api.openai.com/v1"  # Optional override
  api_key: "${OPENAI_API_KEY}"         # Environment variable
  max_retries: 3
  request_delay: 2s
  timeout: 60s

database:
  path: "./llm-verifier.db"
  encryption_key: ""
  backup_enabled: true
  backup_interval: 24h

api:
  port: "8080"
  jwt_secret: "your-jwt-secret"
  rate_limit: 50
  enable_cors: true
  cors_origins:
    - "http://localhost:3000"
    - "https://yourdomain.com"

monitoring:
  enabled: true
  metrics_endpoint: "/metrics"
  health_endpoint: "/health"
  prometheus_port: 9090

performance:
  cache_ttl: 24h
  max_concurrent_requests: 10
  request_timeout: 120s

brotli:
  enabled: true
  auto_detection: true
  cache_enabled: true
  monitor_performance: true

llms:
  - name: "gpt-4"
    endpoint: "https://api.openai.com/v1"
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-4"
    features:
      - streaming
      - function-calling
      - vision
      - brotli
    priority: 1
    max_retries: 3
    timeout: 60s

  - name: "claude-3-opus"
    endpoint: "https://api.anthropic.com/v1"
    api_key: "${ANTHROPIC_API_KEY}"
    model: "claude-3-opus"
    features:
      - streaming
      - function-calling
      - vision
      - brotli
    priority: 2
    max_retries: 3
    timeout: 60s

concurrency: 2
verbose: false
```

---

## üîç Verification

### Verification Criteria

#### 1. Existence Test
**Purpose**: Verify model is accessible and available  
**Method**: HTTP HEAD request to model endpoint  
**Success**: HTTP 200 OK  
**Failure**: HTTP 4xx, 5xx  
**Example**:
```bash
llm-verifier check-existence \
  --provider openai \
  --model gpt-4
```

#### 2. Responsiveness Test
**Purpose**: Verify model responds to requests in acceptable time  
**Method**: HTTP POST with test prompt  
**Metrics**:
- TTFT (Time to First Token): < 10 seconds
- Total Response Time: < 60 seconds
- Response Quality: Valid JSON, no errors

**Example**:
```bash
llm-verifier check-responsiveness \
  --provider anthropic \
  --model claude-3-opus \
  --timeout 30s
```

#### 3. Latency Test
**Purpose**: Measure response performance  
**Method**: Multiple requests, average calculation  
**Metrics**:
- Average TTFT: Time to first token
- P95 Latency: 95th percentile response time
- P99 Latency: 99th percentile response time

**Performance Ratings**:
- Excellent: < 100ms
- Very Good: 100-150ms
- Good: 150-200ms
- Fair: 200-300ms
- Poor: > 300ms

**Example**:
```bash
llm-verifier benchmark-latency \
  --provider google \
  --model gemini-pro \
  --samples 100
```

#### 4. Feature Testing

**Streaming Support**: Verify chunked responses  
**Function Calling**: Test tool execution capabilities  
**Vision Support**: Test image processing  
**Embeddings**: Test vector generation  
**Brotli Compression**: Test HTTP compression support

**Example**:
```bash
llm-verifier check-features \
  --provider openai \
  --model gpt-4 \
  --features streaming,function-calling,vision,brotli
```

---

## üìä Monitoring

### Metrics Dashboard

Access real-time metrics:

```bash
# View all metrics
curl http://localhost:8080/metrics

# View Brotli-specific metrics
curl http://localhost:8080/metrics | grep brotli
```

### Available Metrics

**Brotli Metrics**:
- `llm_verifier_brotli_tests_performed` - Total Brotli detection tests
- `llm_verifier_brotli_supported_models` - Models supporting Brotli
- `llm_verifier_brotli_support_rate_percent` - Percentage of Brotli support
- `llm_verifier_brotli_cache_hits` - Cache hit count
- `llm_verifier_brotli_cache_misses` - Cache miss count
- `llm_verifier_brotli_cache_hit_rate` - Cache hit rate percentage
- `llm_verifier_brotli_avg_detection_time_seconds` - Average detection time

**Performance Metrics**:
- `llm_verifier_api_requests_total` - Total API requests
- `llm_verifier_api_response_time_seconds` - Average response time
- `llm_verifier_verification_success` - Verification success count
- `llm_verifier_verification_failure` - Verification failure count

### Grafana Dashboard

**Setup**:
```bash
# Start Grafana
docker run -d -p 3000:3000 --name=grafana grafana/grafana

# Import dashboard
# Navigate to http://localhost:3000
# Dashboards ‚Üí Import
# Upload: llm-verifier/monitoring/grafana/brotli_dashboard.json
```

---

## üóúÔ∏è Brotli Compression

### Overview

Brotli compression provides **60-70% bandwidth reduction** with minimal latency impact.

### Benefits

- **Bandwidth Savings**: 35% average reduction
- **Faster Transmission**: 40-50% transfer time improvement
- **Cost Reduction**: Reduced API costs from bandwidth savings
- **Better UX**: Faster response loading

### Usage

**Automatic Detection**: System automatically detects Brotli support  
**Caching**: 24-hour TTL reduces unnecessary API calls  
**Configuration**: Enable/disable via config file

### Verification

```bash
# Test Brotli support
llm-verifier test-brotli \
  --provider openai \
  --model gpt-4 \
  --api-key sk-...

# Check cache status
llm-verifier brotli-cache-status
```

### Performance Impact

| Scenario | Without Brotli | With Brotli | Improvement |
|----------|----------------|-------------|-------------|
| Small Request (1KB) | 50ms | 30ms | 40% faster |
| Medium Request (10KB) | 200ms | 120ms | 40% faster |
| Large Request (100KB) | 1000ms | 600ms | 40% faster |
| Average | 416ms | 250ms | **40% overall improvement** |

---

## üìñ API Reference

### CLI Commands

#### Verification Commands

```bash
# Verify model
llm-verifier verify \
  --provider <provider> \
  --model <model-id> \
  --api-key <key> \
  --check-existence \
  --check-responsiveness \
  --check-features \
  --check-brotli

# Verify all configured models
llm-verifier verify-all \
  --config config.yaml \
  --parallel-concurrency 4

# Benchmark performance
llm-verifier benchmark \
  --provider <provider> \
  --model <model-id> \
  --samples 100 \
  --duration 60s

# Generate report
llm-verifier report \
  --format markdown \
  --output ./reports/
```

#### Configuration Commands

```bash
# Export Crush config
llm-verifier export crush \
  --with-api-keys \
  --brotli-only \
  --output crush_config.json

# Export OpenCode config
llm-verifier export opencode \
  --redacted \
  --output opencode_config.json

# Generate discovery
llm-verifier discovery \
  --providers openai,anthropic \
  --output discovery.json
```

#### Monitoring Commands

```bash
# Start monitoring server
llm-verifier server \
  --config config.yaml \
  --port 8080

# Run health checks
llm-verifier health-check \
  --endpoint http://localhost:8080/health

# View metrics
llm-verifier metrics \
  --format prometheus \
  --watch

# Run benchmarks
llm-verifier benchmark \
  --type brotli \
  --iterations 1000
```

---

## üîß Troubleshooting

### Common Issues

#### 1. API Rate Limiting

**Symptom**: HTTP 429 errors  
**Solution**:
```yaml
# Increase retry delay in config
global:
  request_delay: 5s  # Increase from 2s
  max_retries: 5      # Increase from 3
```

#### 2. Model Not Found

**Symptom**: HTTP 404 on model requests  
**Solution**:
- Verify model ID spelling
- Check if model is deprecated
- Update configuration with valid model

#### 3. Timeout Errors

**Symptom**: Requests timing out after 60 seconds  
**Solution**:
```yaml
# Increase timeout in config
llms:
  - name: "gpt-4"
    timeout: 120s  # Increase from 60s
```

#### 4. Brotli Detection Failing

**Symptom**: Brotli tests returning false for all models  
**Solution**:
- Check API key validity
- Verify network connectivity
- Clear Brotli cache: `llm-verifier clear-brotli-cache`
- Run with verbose mode: `llm-verifier verify --verbose`

#### 5. Database Lock Errors

**Symptom**: "database is locked" errors  
**Solution**:
```bash
# Check for other running instances
ps aux | grep llm-verifier

# Kill stuck processes
killall llm-verifier

# Reset database (backup first!)
llm-verifier database-backup --output backup.db
llm-verifier database-reset
```

#### 6. Migration Failures

**Symptom**: Database migration errors  
**Solution**:
```bash
# Check current version
llm-verifier db-version

# Force re-run migrations
llm-verifier migrate --force

# Reset database (last resort)
llm-verifier database-reset --force
```

### Debug Mode

Enable verbose logging:

```bash
# Run with debug output
llm-verifier --verbose verify --config config.yaml

# Set log level
export LLM_VERIFIER_LOG_LEVEL=debug
llm-verifier server --config config.yaml

# Enable trace logging
export LLM_VERIFIER_TRACE=true
llm-verifier verify --provider openai --model gpt-4
```

---

## üöÄ Advanced Features

### Multi-Provider Verification

Verify models across multiple providers simultaneously:

```bash
llm-verifier verify-all \
  --providers openai,anthropic,google \
  --parallel 4 \
  --timeout 60s \
  --output ./multi-provider-results/
```

### Batch Processing

Process multiple models efficiently:

```bash
# Create batch file
cat > batch.yaml << EOF
verification:
  - provider: openai
    models: [gpt-4, gpt-3.5-turbo]
    features: [streaming, function-calling, brotli]
  - provider: anthropic
    models: [claude-3-opus, claude-3-sonnet]
    features: [streaming, function-calling, vision, brotli]

# Run batch verification
llm-verifier batch-verify --batch-file batch.yaml
```

### Continuous Monitoring

Set up automated monitoring:

```bash
# Start monitoring daemon
llm-verifier monitor-daemon \
  --interval 60s \
  --alert-threshold 0.95 \
  --webhook https://your-webhook.com/alerts

# Monitor specific metrics
llm-verifier monitor \
  --metrics brotli,api,verification \
  --output ./monitoring-output/
```

### Custom Verification Scripts

Create custom verification workflows:

```bash
# Example: Verify only Brotli support
cat > verify-brotli.sh << 'EOF_SCRIPT'
#!/bin/bash
llm-verifier verify-all \
  --config config.yaml \
  --feature-filter brotli \
  --output brotli-results/

# Check success rate
jq '.[].success_rate' brotli-results/report.json
EOF_SCRIPT

chmod +x verify-brotli.sh
./verify-brotli.sh
```

---

## üìö Additional Resources

### Documentation

- [API Documentation](./api/docs/README.md)
- [Brotli Implementation Guide](../BROTLI_IMPLEMENTATION_COMPLETE.md)
- [Deployment Guide](../BROTLI_DEPLOYMENT_GUIDE.md)
- [Architecture Overview](../llm-verifier/docs/ARCHITECTURE_OVERVIEW.md)

### Configuration Examples

- [Example Configuration](../config.yaml.example)
- [Real-world Brotli Test](../real_world_brotli_test.yaml)
- [Brotli Test Config](../brotli_test_config.yaml)

### Exported Configurations

Download verified configurations:
- [Crush Configuration (Full)](./Downloads/LLM-Verifier-Reports/verified_crush_config.json)
- [OpenCode Configuration](./Downloads/LLM-Verifier-Reports/verified_opencode_config.json)
- [Crush Configuration (Brotli Optimized)](./Downloads/LLM-Verifier-Reports/verified_crush_brotli_optimized.json)

---

## üéì Best Practices

### For Production

1. **Use Brotli-Optimized Configs** - 74.8% of models support compression
2. **Enable Caching** - Reduces API calls by 80%
3. **Monitor Cache Hit Rate** - Aim for >80% cache hit rate
4. **Set Appropriate Timeouts** - Match model capabilities
5. **Implement Retry Logic** - Exponential backoff for rate limits
6. **Use Multiple Providers** - Redundancy for high availability
7. **Track Performance Metrics** - Monitor latency and success rates
8. **Regular Verification** - Re-verify models weekly

### For Development

1. **Start with Small Batches** - Test 5-10 models initially
2. **Use Verbose Mode** - Understand verification behavior
3. **Check Logs Regularly** - Monitor for issues
4. **Use Redacted Configs** - Never commit API keys
5. **Run Benchmarks** - Compare provider performance
6. **Test Brotli Separately** - Verify compression impact

---

## üìû Support

### Getting Help

```bash
# Help command
llm-verifier --help

# Specific command help
llm-verifier verify --help
llm-verifier export --help
```

### Version Information

```bash
# Check version
llm-verifier --version

# System information
llm-verifier system-info

# Diagnostic output
llm-verifier diagnose
```

### Bug Reporting

Found an issue? Report with:

```bash
# Generate bug report
llm-verifier bug-report \
  --description "<issue description>" \
  --output bug-report.json

# Include logs
llm-verifier logs --last 100 --output logs.txt
```

---

**User Manual Version**: 1.0.0  
**Last Updated**: December 25, 2025  
**For latest documentation**: https://github.com/your-org/llm-verifier
