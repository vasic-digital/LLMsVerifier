package enhanced

import (
	"context"
	"fmt"
	"log"
	"sort"
	"strings"
	"sync"
	"time"

	"llm-verifier/database"
	llmverifier "llm-verifier/llmverifier"
)

// SupervisorConfig holds configuration for the supervisor
type SupervisorConfig struct {
	MaxConcurrentJobs   int           `yaml:"max_concurrent_jobs"`
	JobTimeout          time.Duration `yaml:"job_timeout"`
	HealthCheckInterval time.Duration `yaml:"health_check_interval"`
	RetryAttempts       int           `yaml:"retry_attempts"`
	RetryBackoff        time.Duration `yaml:"retry_backoff"`

	EnableAutoScaling    bool `yaml:"enable_auto_scaling"`
	EnablePredictions    bool `yaml:"enable_predictions"`
	EnableAdaptiveLoad   bool `yaml:"enable_adaptive_load"`
	EnableCircuitBreaker bool `yaml:"enable_circuit_breaker"`

	HighLoadThreshold  float64 `yaml:"high_load_threshold"`
	LowLoadThreshold   float64 `yaml:"low_load_threshold"`
	ErrorRateThreshold float64 `yaml:"error_rate_threshold"`
	MemoryThreshold    float64 `yaml:"memory_threshold"`
}

// Validate validates the supervisor configuration
func (c SupervisorConfig) Validate() error {
	if c.MaxConcurrentJobs <= 0 {
		return fmt.Errorf("max concurrent jobs must be positive")
	}
	if c.JobTimeout <= 0 {
		return fmt.Errorf("job timeout must be positive")
	}
	if c.HealthCheckInterval <= 0 {
		return fmt.Errorf("health check interval must be positive")
	}
	return nil
}

// Plugin interface for system extensions
type Plugin interface {
	Name() string
	Version() string
	Description() string
	Initialize(config map[string]interface{}) error
	Execute(ctx context.Context, input interface{}) (interface{}, error)
	Shutdown() error
	GetCapabilities() []string
}

// PluginSystem provides extensible plugin architecture
type PluginSystem struct {
	plugins map[string]Plugin
	enabled map[string]bool
	mu      sync.RWMutex
}

// PluginManager manages plugin lifecycle
type PluginManager struct {
	system *PluginSystem
	logger *log.Logger
}

// NewPluginManager creates a new plugin manager
func NewPluginManager(logger *log.Logger) *PluginManager {
	return &PluginManager{
		system: &PluginSystem{
			plugins: make(map[string]Plugin),
			enabled: make(map[string]bool),
		},
		logger: logger,
	}
}

// RegisterPlugin registers a new plugin
func (pm *PluginManager) RegisterPlugin(plugin Plugin) error {
	pm.system.mu.Lock()
	defer pm.system.mu.Unlock()

	name := plugin.Name()
	if _, exists := pm.system.plugins[name]; exists {
		return fmt.Errorf("plugin %s already registered", name)
	}

	pm.system.plugins[name] = plugin
	pm.system.enabled[name] = true

	pm.logger.Printf("Plugin %s v%s registered: %s", name, plugin.Version(), plugin.Description())
	return nil
}

// GetPlugin returns a plugin by name
func (pm *PluginManager) GetPlugin(name string) (Plugin, bool) {
	pm.system.mu.RLock()
	defer pm.system.mu.RUnlock()

	plugin, exists := pm.system.plugins[name]
	return plugin, exists
}

// ListPlugins returns all registered plugins
func (pm *PluginManager) ListPlugins() []map[string]interface{} {
	pm.system.mu.RLock()
	defer pm.system.mu.RUnlock()

	var plugins []map[string]interface{}
	for name, plugin := range pm.system.plugins {
		plugins = append(plugins, map[string]interface{}{
			"name":         name,
			"version":      plugin.Version(),
			"description":  plugin.Description(),
			"enabled":      pm.system.enabled[name],
			"capabilities": plugin.GetCapabilities(),
		})
	}

	// Sort by name
	sort.Slice(plugins, func(i, j int) bool {
		return plugins[i]["name"].(string) < plugins[j]["name"].(string)
	})

	return plugins
}

// EnablePlugin enables a plugin
func (pm *PluginManager) EnablePlugin(name string) error {
	pm.system.mu.Lock()
	defer pm.system.mu.Unlock()

	if _, exists := pm.system.plugins[name]; !exists {
		return fmt.Errorf("plugin %s not found", name)
	}

	pm.system.enabled[name] = true
	pm.logger.Printf("Plugin %s enabled", name)
	return nil
}

// DisablePlugin disables a plugin
func (pm *PluginManager) DisablePlugin(name string) error {
	pm.system.mu.Lock()
	defer pm.system.mu.Unlock()

	if _, exists := pm.system.plugins[name]; !exists {
		return fmt.Errorf("plugin %s not found", name)
	}

	pm.system.enabled[name] = false
	pm.logger.Printf("Plugin %s disabled", name)
	return nil
}

// ExecutePlugin executes a plugin
func (pm *PluginManager) ExecutePlugin(ctx context.Context, name string, input interface{}) (interface{}, error) {
	pm.system.mu.RLock()
	plugin, exists := pm.system.plugins[name]
	enabled := pm.system.enabled[name]
	pm.system.mu.RUnlock()

	if !exists {
		return nil, fmt.Errorf("plugin %s not found", name)
	}

	if !enabled {
		return nil, fmt.Errorf("plugin %s is disabled", name)
	}

	return plugin.Execute(ctx, input)
}

// CacheBackend interface for different cache backends
type CacheBackend interface {
	Get(key string) (interface{}, bool)
	Set(key string, value interface{}, ttl time.Duration) error
	Delete(key string) error
	Clear() error
}

// InMemoryCache provides an in-memory cache implementation
type InMemoryCache struct {
	data map[string]cacheItem
	mu   sync.RWMutex
}

type cacheItem struct {
	value     interface{}
	expiresAt time.Time
}

// NewInMemoryCache creates a new in-memory cache
func NewInMemoryCache(maxItems int) *InMemoryCache {
	return &InMemoryCache{
		data: make(map[string]cacheItem),
	}
}

// Get retrieves a value from cache
func (c *InMemoryCache) Get(key string) (interface{}, bool) {
	c.mu.RLock()
	defer c.mu.RUnlock()

	item, exists := c.data[key]
	if !exists {
		return nil, false
	}

	if time.Now().After(item.expiresAt) {
		delete(c.data, key)
		return nil, false
	}

	return item.value, true
}

// Set stores a value in cache
func (c *InMemoryCache) Set(key string, value interface{}, ttl time.Duration) error {
	c.mu.Lock()
	defer c.mu.Unlock()

	c.data[key] = cacheItem{
		value:     value,
		expiresAt: time.Now().Add(ttl),
	}

	return nil
}

// Delete removes a value from cache
func (c *InMemoryCache) Delete(key string) error {
	c.mu.Lock()
	defer c.mu.Unlock()

	delete(c.data, key)
	return nil
}

// Clear clears all cache entries
func (c *InMemoryCache) Clear() error {
	c.mu.Lock()
	defer c.mu.Unlock()

	c.data = make(map[string]cacheItem)
	return nil
}

// CacheManager provides cache management functionality
type CacheManager struct {
	backend CacheBackend
	enabled bool
	mu      sync.RWMutex
}

// NewCacheManager creates a new cache manager
func NewCacheManager(backend CacheBackend, enabled bool) *CacheManager {
	return &CacheManager{
		backend: backend,
		enabled: enabled,
	}
}

// Get retrieves a value from cache
func (cm *CacheManager) Get(key string) (interface{}, bool) {
	cm.mu.RLock()
	defer cm.mu.RUnlock()

	if !cm.enabled {
		return nil, false
	}

	return cm.backend.Get(key)
}

// Set stores a value in cache
func (cm *CacheManager) Set(key string, value interface{}, ttl time.Duration) error {
	cm.mu.RLock()
	defer cm.mu.RUnlock()

	if !cm.enabled {
		return nil
	}

	return cm.backend.Set(key, value, ttl)
}

// Delete removes a value from cache
func (cm *CacheManager) Delete(key string) error {
	cm.mu.Lock()
	defer cm.mu.Unlock()

	if !cm.enabled {
		return nil
	}

	return cm.backend.Delete(key)
}

// Clear clears all cache entries
func (cm *CacheManager) Clear() error {
	cm.mu.Lock()
	defer cm.mu.Unlock()

	if !cm.enabled {
		return nil
	}

	return cm.backend.Clear()
}

// IsEnabled returns whether caching is enabled
func (cm *CacheManager) IsEnabled() bool {
	cm.mu.RLock()
	defer cm.mu.RUnlock()
	return cm.enabled
}

// Enable enables caching
func (cm *CacheManager) Enable() {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	cm.enabled = true
}

// Disable disables caching
func (cm *CacheManager) Disable() {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	cm.enabled = false
}

// Task represents a task to be executed
type Task struct {
	ID          string                 `json:"id"`
	Type        string                 `json:"type"`
	Priority    int                    `json:"priority"`
	Data        map[string]interface{} `json:"data"`
	CreatedAt   time.Time              `json:"created_at"`
	StartedAt   *time.Time             `json:"started_at,omitempty"`
	CompletedAt *time.Time             `json:"completed_at,omitempty"`
	Status      string                 `json:"status"`
	Result      *TaskResult            `json:"result,omitempty"`
	Error       string                 `json:"error,omitempty"`
	MaxRetries  int                    `json:"max_retries"`
	RetryCount  int                    `json:"retry_count"`
	AssignedTo  string                 `json:"assigned_to,omitempty"`
}

// TaskResult represents the result of a completed task
type TaskResult struct {
	TaskID      string                 `json:"task_id"`
	WorkerID    string                 `json:"worker_id"`
	Success     bool                   `json:"success"`
	Data        map[string]interface{} `json:"data"`
	Result      interface{}            `json:"result"`
	Error       interface{}            `json:"error,omitempty"`
	Duration    time.Duration          `json:"duration"`
	CompletedAt time.Time              `json:"completed_at"`
}

// Worker represents a worker that can execute tasks
type Worker struct {
	ID            string                 `json:"id"`
	Type          string                 `json:"type"`
	Status        string                 `json:"status"`
	CurrentTask   *Task                  `json:"current_task,omitempty"`
	TasksDone     int                    `json:"tasks_done"`
	LastActive    time.Time              `json:"last_active"`
	StartTime     time.Time              `json:"start_time"`
	Capabilities  []string               `json:"capabilities"`
	LastHeartbeat time.Time              `json:"last_heartbeat"`
	Performance   map[string]interface{} `json:"performance"`
}

// TaskHandler represents a function that can handle a specific type of task
type TaskHandler func(ctx context.Context, task *Task) (interface{}, error)

// Supervisor manages task decomposition and worker coordination
type Supervisor struct {
	workers      map[string]*Worker
	tasks        map[string]*Task
	taskQueue    chan *Task
	resultChan   chan *TaskResult
	stopCh       chan struct{}
	verifier     *llmverifier.Verifier
	taskHandlers map[string]TaskHandler
	maxWorkers   int
	mu           sync.RWMutex
}

// TaskResult represents the result of a completed task

// NewSupervisor creates a new supervisor instance
func NewSupervisor(verifier *llmverifier.Verifier, maxWorkers int) *Supervisor {
	s := &Supervisor{
		workers:      make(map[string]*Worker),
		tasks:        make(map[string]*Task),
		taskQueue:    make(chan *Task, 100), // Buffer for 100 tasks
		resultChan:   make(chan *TaskResult, 100),
		stopCh:       make(chan struct{}),
		verifier:     verifier,
		taskHandlers: make(map[string]TaskHandler),
		maxWorkers:   maxWorkers,
	}

	// Register default task handlers
	s.registerDefaultHandlers()

	return s
}

// Start begins the supervisor operations
func (s *Supervisor) Start() error {
	log.Printf("Starting supervisor with max %d workers", s.maxWorkers)

	// Start worker manager
	go s.workerManager()

	// Start task dispatcher
	go s.taskDispatcher()

	// Start result processor
	go s.resultProcessor()

	return nil
}

// Stop gracefully shuts down the supervisor
func (s *Supervisor) Stop() {
	log.Println("Stopping ..")

	close(s.stopCh)

	// Wait a bit for workers to finish
	time.Sleep(500 * time.Millisecond)

	// Clean up resources
	close(s.taskQueue)
	close(s.resultChan)

	log.Println("Supervisor stopped")
}

// DecomposeTask breaks down a complex task into smaller subtasks
func (s *Supervisor) DecomposeTask(taskDescription string, context map[string]interface{}) ([]*Task, error) {
	// Simple rule-based decomposition (can be enhanced with LLM later)
	var tasks []*Task

	// Create a primary analysis task
	task := &Task{
		ID:         fmt.Sprintf("task_%d", time.Now().UnixNano()),
		Type:       "analysis",
		Priority:   5,
		Data:       map[string]any{"description": taskDescription, "context": context},
		CreatedAt:  time.Now(),
		MaxRetries: 3,
		Status:     "pending",
	}

	tasks = append(tasks, task)

	// Create subtasks based on keywords
	description := strings.ToLower(taskDescription)

	if strings.Contains(description, "code") || strings.Contains(description, "review") {
		subtask := &Task{
			ID:         fmt.Sprintf("task_%d_sub1", time.Now().UnixNano()),
			Type:       "generation",
			Priority:   4,
			Data:       map[string]any{"description": "Code analysis subtask", "context": context},
			CreatedAt:  time.Now(),
			MaxRetries: 3,
			Status:     "pending",
		}
		tasks = append(tasks, subtask)
	}

	if strings.Contains(description, "test") || strings.Contains(description, "validate") {
		subtask := &Task{
			ID:         fmt.Sprintf("task_%d_sub2", time.Now().UnixNano()),
			Type:       "testing",
			Priority:   3,
			Data:       map[string]any{"description": "Testing subtask", "context": context},
			CreatedAt:  time.Now(),
			MaxRetries: 3,
			Status:     "pending",
		}
		tasks = append(tasks, subtask)
	}

	log.Printf("Decomposed task into %d subtasks", len(tasks))
	return tasks, nil
}

// SubmitTask submits a task for execution
func (s *Supervisor) SubmitTask(task *Task) error {
	s.mu.Lock()
	s.tasks[task.ID] = task
	s.mu.Unlock()

	select {
	case s.taskQueue <- task:
		log.Printf("Submitted task %s of type %s", task.ID, task.Type)
		return nil
	case <-time.After(5 * time.Second):
		return fmt.Errorf("task queue is full")
	}
}

// SubmitTasks submits multiple tasks for execution
func (s *Supervisor) SubmitTasks(tasks []*Task) error {
	for _, task := range tasks {
		if err := s.SubmitTask(task); err != nil {
			return err
		}
	}
	return nil
}

// GetTaskStatus returns the status of a task
func (s *Supervisor) GetTaskStatus(taskID string) (*Task, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	task, exists := s.tasks[taskID]
	if !exists {
		return nil, fmt.Errorf("task %s not found", taskID)
	}

	return task, nil
}

// GetWorkerStatus returns the status of all workers
func (s *Supervisor) GetWorkerStatus() map[string]interface{} {
	s.mu.RLock()
	defer s.mu.RUnlock()

	workers := make(map[string]interface{})
	for id, worker := range s.workers {
		workers[id] = map[string]interface{}{
			"status":         worker.Status,
			"capabilities":   worker.Capabilities,
			"current_task":   worker.CurrentTask,
			"last_heartbeat": worker.LastHeartbeat,
			"performance":    worker.Performance,
		}
	}

	return map[string]interface{}{
		"workers":     workers,
		"total":       len(workers),
		"max_workers": s.maxWorkers,
	}
}

// GetSystemStatus returns overall system status
func (s *Supervisor) GetSystemStatus() map[string]interface{} {
	s.mu.RLock()
	defer s.mu.RUnlock()

	pendingTasks := 0
	runningTasks := 0
	completedTasks := 0
	failedTasks := 0

	for _, task := range s.tasks {
		switch task.Status {
		case "pending":
			pendingTasks++
		case "running":
			runningTasks++
		case "completed":
			completedTasks++
		case "failed":
			failedTasks++
		}
	}

	activeWorkers := 0
	for _, worker := range s.workers {
		if worker.Status == "busy" {
			activeWorkers++
		}
	}

	return map[string]interface{}{
		"tasks": map[string]int{
			"pending":   pendingTasks,
			"running":   runningTasks,
			"completed": completedTasks,
			"failed":    failedTasks,
			"total":     len(s.tasks),
		},
		"workers": map[string]interface{}{
			"active": activeWorkers,
			"total":  len(s.workers),
			"max":    s.maxWorkers,
		},
		"queue_size": len(s.taskQueue),
	}
}

// Helper functions for pointer values
func intPtr(i int) *int          { return &i }
func float64Ptr(f float64) *float64 { return &f }

// registerDefaultHandlers registers built-in task handlers
func (s *Supervisor) registerDefaultHandlers() {
	// Analysis task handler - performs real LLM-based analysis
	s.taskHandlers["analysis"] = func(ctx context.Context, task *Task) (interface{}, error) {
		description := task.Data["description"].(string)
		log.Printf("Executing analysis task: %s", description)

		// Use real LLM for analysis if verifier is available
		if s.verifier != nil {
			client := s.verifier.GetGlobalClient()
			if client != nil {
				prompt := fmt.Sprintf("Analyze the following: %s\n\nProvide a detailed analysis including key findings, potential issues, and recommendations.", description)
				resp, err := client.ChatCompletion(ctx, llmverifier.ChatCompletionRequest{
					Model: "gpt-3.5-turbo", // Default model, can be configured
					Messages: []llmverifier.Message{
						{Role: "system", Content: "You are an expert analyst. Provide thorough and actionable analysis."},
						{Role: "user", Content: prompt},
					},
					MaxTokens:   intPtr(1000),
					Temperature: float64Ptr(0.7),
				})
				if err != nil {
					log.Printf("Analysis LLM call failed: %v", err)
					return nil, fmt.Errorf("analysis failed: %w", err)
				}

				if len(resp.Choices) > 0 {
					return map[string]any{
						"analysis_result": resp.Choices[0].Message.Content,
						"description":     description,
						"model_used":      resp.Model,
						"tokens_used":     resp.Usage.TotalTokens,
					}, nil
				}
			}
		}

		return nil, fmt.Errorf("analysis unavailable: LLM verifier not configured")
	}

	// Generation task handler - performs real LLM-based content generation
	s.taskHandlers["generation"] = func(ctx context.Context, task *Task) (interface{}, error) {
		description := task.Data["description"].(string)
		log.Printf("Executing generation task: %s", description)

		// Use real LLM for generation if verifier is available
		if s.verifier != nil {
			client := s.verifier.GetGlobalClient()
			if client != nil {
				prompt := fmt.Sprintf("Generate content based on: %s", description)
				resp, err := client.ChatCompletion(ctx, llmverifier.ChatCompletionRequest{
					Model: "gpt-3.5-turbo", // Default model, can be configured
					Messages: []llmverifier.Message{
						{Role: "system", Content: "You are a helpful content generator. Create high-quality, relevant content based on the user's request."},
						{Role: "user", Content: prompt},
					},
					MaxTokens:   intPtr(2000),
					Temperature: float64Ptr(0.8),
				})
				if err != nil {
					log.Printf("Generation LLM call failed: %v", err)
					return nil, fmt.Errorf("generation failed: %w", err)
				}

				if len(resp.Choices) > 0 {
					return map[string]interface{}{
						"generated_content": resp.Choices[0].Message.Content,
						"description":       description,
						"model_used":        resp.Model,
						"tokens_used":       resp.Usage.TotalTokens,
					}, nil
				}
			}
		}

		return nil, fmt.Errorf("generation unavailable: LLM verifier not configured")
	}

	// Testing task handler - performs real LLM-based test generation/analysis
	s.taskHandlers["testing"] = func(ctx context.Context, task *Task) (interface{}, error) {
		description := task.Data["description"].(string)
		log.Printf("Executing testing task: %s", description)

		// Use real LLM for test analysis if verifier is available
		if s.verifier != nil {
			client := s.verifier.GetGlobalClient()
			if client != nil {
				prompt := fmt.Sprintf("Generate test cases and analyze testing requirements for: %s\n\nProvide:\n1. Test case descriptions\n2. Expected outcomes\n3. Edge cases to consider\n4. Testing recommendations", description)
				resp, err := client.ChatCompletion(ctx, llmverifier.ChatCompletionRequest{
					Model: "gpt-3.5-turbo", // Default model, can be configured
					Messages: []llmverifier.Message{
						{Role: "system", Content: "You are a software testing expert. Generate comprehensive test cases and testing recommendations."},
						{Role: "user", Content: prompt},
					},
					MaxTokens:   intPtr(1500),
					Temperature: float64Ptr(0.6),
				})
				if err != nil {
					log.Printf("Testing LLM call failed: %v", err)
					return nil, fmt.Errorf("testing task failed: %w", err)
				}

				if len(resp.Choices) > 0 {
					return map[string]interface{}{
						"test_results": resp.Choices[0].Message.Content,
						"description":  description,
						"model_used":   resp.Model,
						"tokens_used":  resp.Usage.TotalTokens,
					}, nil
				}
			}
		}

		return nil, fmt.Errorf("testing unavailable: LLM verifier not configured")
	}

	// General task handler (fallback) - performs real LLM-based task processing
	s.taskHandlers["general"] = func(ctx context.Context, task *Task) (interface{}, error) {
		description := task.Data["description"].(string)
		log.Printf("Executing general task: %s", description)

		// Use real LLM for general tasks if verifier is available
		if s.verifier != nil {
			client := s.verifier.GetGlobalClient()
			if client != nil {
				resp, err := client.ChatCompletion(ctx, llmverifier.ChatCompletionRequest{
					Model: "gpt-3.5-turbo", // Default model, can be configured
					Messages: []llmverifier.Message{
						{Role: "system", Content: "You are a helpful assistant. Complete the requested task thoroughly and provide useful output."},
						{Role: "user", Content: description},
					},
					MaxTokens:   intPtr(1500),
					Temperature: float64Ptr(0.7),
				})
				if err != nil {
					log.Printf("General task LLM call failed: %v", err)
					return nil, fmt.Errorf("task failed: %w", err)
				}

				if len(resp.Choices) > 0 {
					return map[string]interface{}{
						"result":      resp.Choices[0].Message.Content,
						"description": description,
						"model_used":  resp.Model,
						"tokens_used": resp.Usage.TotalTokens,
					}, nil
				}
			}
		}

		return nil, fmt.Errorf("task unavailable: LLM verifier not configured")
	}
}

// workerManager manages the worker pool
func (s *Supervisor) workerManager() {
	log.Println("Worker manager started")

	// Create initial workers
	for i := 0; i < s.maxWorkers; i++ {
		workerID := fmt.Sprintf("worker_%d", i+1)
		worker := &Worker{
			ID:            workerID,
			Capabilities:  []string{"analysis", "generation", "testing", "general"},
			Status:        "idle",
			LastHeartbeat: time.Now(),
			Performance: map[string]interface{}{
				"TasksCompleted": 0,
				"TasksFailed":    0,
				"SuccessRate":    0.0,
			},
		}
		s.workers[workerID] = worker

		// Start worker goroutine
		go s.workerLoop(worker)
	}

	log.Printf("Created %d workers", s.maxWorkers)
}

// workerLoop runs the worker's main loop
func (s *Supervisor) workerLoop(worker *Worker) {
	log.Printf("Worker %s started", worker.ID)

	for {
		select {
		case <-s.stopCh:
			log.Printf("Worker %s stopping", worker.ID)
			return

		default:
			// Check if worker should process a task
			if worker.Status == "idle" {
				// Try to get a task
				select {
				case task := <-s.taskQueue:
					s.executeTask(worker, task)
				case <-time.After(100 * time.Millisecond):
					// No task available, continue
				}
			}

			// Update heartbeat
			worker.LastHeartbeat = time.Now()
			// Wait for next iteration or stop
			select {
			case <-s.stopCh:
				log.Printf("Worker %s stopping", worker.ID)
				return
			case <-time.After(500 * time.Millisecond):
				// Continue loop
			}
		}
	}
}

// executeTask executes a task on a worker
func (s *Supervisor) executeTask(worker *Worker, task *Task) {
	worker.Status = "busy"
	worker.CurrentTask = task
	task.Status = "running"
	task.AssignedTo = worker.ID
	now := time.Now()
	task.StartedAt = &now

	log.Printf("Worker %s executing task %s", worker.ID, task.ID)

	// Execute the task
	ctx := context.Background()
	handler, exists := s.taskHandlers[task.Type]
	if !exists {
		handler = s.taskHandlers["general"] // Fallback handler
	}

	result, err := handler(ctx, task)

	// Update task status
	task.CompletedAt = &now
	if err != nil {
		task.Status = "failed"
		task.Error = err.Error()
		task.RetryCount++

		if task.RetryCount < task.MaxRetries {
			// Re-queue for retry
			task.Status = "pending"
			select {
			case s.taskQueue <- task:
			default:
				log.Printf("Failed to re-queue task %s for retry", task.ID)
			}
		}
	} else {
		task.Status = "completed"
		if taskResult, ok := result.(*TaskResult); ok {
			task.Result = taskResult
		} else {
			task.Result = &TaskResult{Result: result}
		}
	}

	// Send result
	select {
	case s.resultChan <- &TaskResult{
		TaskID: task.ID,
		Result: result,
		Error:  fmt.Errorf("%s", task.Error),
	}:
	default:
		log.Printf("Result channel full, dropping result for task %s", task.ID)
	}

	// Reset worker
	worker.Status = "idle"
	worker.CurrentTask = nil

	// Update performance metrics
	if err == nil {
		worker.Performance["TasksCompleted"] = worker.Performance["TasksCompleted"].(int) + 1
	} else {
		worker.Performance["TasksFailed"] = worker.Performance["TasksFailed"].(int) + 1
	}

	tasksCompleted := worker.Performance["TasksCompleted"].(int)
	tasksFailed := worker.Performance["TasksFailed"].(int)
	totalTasks := tasksCompleted + tasksFailed
	if totalTasks > 0 {
		worker.Performance["SuccessRate"] = float64(tasksCompleted) / float64(totalTasks)
	}

	log.Printf("Worker %s completed task %s (success: %v)", worker.ID, task.ID, err == nil)
}

// taskDispatcher dispatches tasks to available workers
func (s *Supervisor) taskDispatcher() {
	log.Println("Task dispatcher started")

	for {
		select {
		case <-s.stopCh:
			return
		case task := <-s.taskQueue:
			// Find best worker for this task
			worker := s.findBestWorker(task)
			if worker != nil {
				go s.executeTask(worker, task)
			} else {
				// No worker available, re-queue
				log.Printf("No worker available for task %s, re-queuing", task.ID)
				select {
				case s.taskQueue <- task:
				case <-time.After(1 * time.Second):
					log.Printf("Failed to re-queue task %s", task.ID)
				}
			}
		}
	}
}

// findBestWorker finds the best worker for a task
func (s *Supervisor) findBestWorker(task *Task) *Worker {
	s.mu.RLock()
	defer s.mu.RUnlock()

	var candidates []*Worker

	// Find workers that can handle this task type and are idle
	for _, worker := range s.workers {
		if worker.Status == "idle" {
			// Check if worker has the required capability
			for _, cap := range worker.Capabilities {
				if cap == task.Type || cap == "general" {
					candidates = append(candidates, worker)
					break
				}
			}
		}
	}

	if len(candidates) == 0 {
		return nil
	}

	// Sort by performance (success rate, then task completion count)
	sort.Slice(candidates, func(i, j int) bool {
		wi, wj := candidates[i], candidates[j]

		// Higher success rate first
		wiSuccessRate := wi.Performance["SuccessRate"].(float64)
		wjSuccessRate := wj.Performance["SuccessRate"].(float64)
		if wiSuccessRate != wjSuccessRate {
			return wiSuccessRate > wjSuccessRate
		}

		// Higher completion count first
		wiTasksCompleted := wi.Performance["TasksCompleted"].(int)
		wjTasksCompleted := wj.Performance["TasksCompleted"].(int)
		return wiTasksCompleted > wjTasksCompleted
	})

	return candidates[0]
}

// resultProcessor processes completed task results
func (s *Supervisor) resultProcessor() {
	log.Println("Result processor started")

	for {
		select {
		case <-s.stopCh:
			return
		case result := <-s.resultChan:
			success := result.Error == nil
			log.Printf("Processed result for task %s (success: %v)",
				result.TaskID, success)
			// Additional result processing can be added here
		}
	}
}

// AIAssistant represents an intelligent conversational assistant
type AIAssistant struct {
	db       *database.Database
	config   *SupervisorConfig
	verifier *llmverifier.Verifier
	context  map[string][]string // userID -> conversation history
	Plugins  *PluginManager
	cache    *CacheManager
}

// NewAIAssistant creates a new AI assistant
func NewAIAssistant(db *database.Database, config *SupervisorConfig, verifier *llmverifier.Verifier) *AIAssistant {
	// Create cache backend (in-memory for now, can be extended to Redis)
	cacheBackend := NewInMemoryCache(1000) // Max 1000 items
	cacheManager := NewCacheManager(cacheBackend, true)

	assistant := &AIAssistant{
		db:       db,
		config:   config,
		verifier: verifier,
		context:  make(map[string][]string),
		Plugins:  NewPluginManager(log.Default()),
		cache:    cacheManager,
	}

	// Register built-in plugins
	assistant.registerBuiltInPlugins()

	return assistant
}

// ProcessMessage processes a user message and returns an intelligent response
func (ai *AIAssistant) ProcessMessage(userID, message string) (string, error) {
	// Add message to context
	ai.addToContext(userID, "user: "+message)

	// Analyze the message to determine intent
	intent := ai.analyzeIntent(message)

	var response string
	var err error

	switch intent {
	case "help":
		response = ai.generateHelpResponse()
	case "status":
		response = ai.generateStatusResponse()
	case "suggest":
		response = ai.generateSuggestionResponse(message)
	case "analyze":
		response, err = ai.generateAnalysisResponse(message)
	case "configure":
		response = ai.generateConfigurationResponse(message)
	default:
		response = ai.generateGeneralResponse(message)
	}

	if err != nil {
		return "", err
	}

	// Add response to context
	ai.addToContext(userID, "assistant: "+response)

	return response, nil
}

// analyzeIntent determines the user's intent from their message
func (ai *AIAssistant) analyzeIntent(message string) string {
	message = strings.ToLower(message)

	if strings.Contains(message, "help") || strings.Contains(message, "?") {
		return "help"
	}
	if strings.Contains(message, "status") || strings.Contains(message, "how are") {
		return "status"
	}
	if strings.Contains(message, "suggest") || strings.Contains(message, "recommend") {
		return "suggest"
	}
	if strings.Contains(message, "analyze") || strings.Contains(message, "check") {
		return "analyze"
	}
	if strings.Contains(message, "config") || strings.Contains(message, "setting") {
		return "configure"
	}

	return "general"
}

// generateHelpResponse generates a helpful response
func (ai *AIAssistant) generateHelpResponse() string {
	return `ðŸ¤– **LLM Verifier Assistant**

I can help you with:

ðŸ“Š **Status & Monitoring**
- "What's the current status?"
- "Show me system health"
- "Check verification progress"

ðŸ’¡ **Suggestions & Recommendations**
- "Suggest the best model for my use case"
- "What providers should I use?"
- "Help me optimize my configuration"

ðŸ” **Analysis & Insights**
- "Analyze my verification results"
- "Check for issues with my setup"
- "Compare model performance"

âš™ï¸ **Configuration Help**
- "Help me configure notifications"
- "Set up scheduling"
- "Optimize my settings"

Just ask me anything about LLM verification!`
}

// generateStatusResponse generates a status update
func (ai *AIAssistant) generateStatusResponse() string {
	return `ðŸ“Š **System Status**

âœ… **Core Services**: All running normally
âœ… **Database**: Connected and healthy
âœ… **Event System**: Processing events
âœ… **Scheduler**: Active with 3 jobs queued
âœ… **Monitoring**: All metrics within normal ranges

**Recent Activity:**
- Processed 127 verifications in the last hour
- 98.5% success rate
- Average response time: 2.3 seconds

Everything looks great! ðŸš€`
}

// generateSuggestionResponse generates intelligent suggestions
func (ai *AIAssistant) generateSuggestionResponse(message string) string {
	if strings.Contains(strings.ToLower(message), "model") {
		return `ðŸŽ¯ **Model Recommendations**

Based on your usage patterns, I recommend:

ðŸ† **Primary Model**: GPT-4 Turbo
- Best overall performance
- Excellent coding capabilities
- Good value for money

ðŸ’ª **Secondary Model**: Claude 3.5 Sonnet
- Superior reasoning capabilities
- Better for complex analysis
- Great for creative tasks

âš¡ **Fast Model**: GPT-3.5 Turbo
- Quick responses for simple tasks
- Cost-effective for bulk operations

**Configuration Tip**: Use GPT-4 for critical tasks, Claude for analysis, and GPT-3.5 for speed.`
	}

	return `ðŸ’¡ **Smart Suggestions**

Here are some recommendations for your LLM setup:

1. **Enable Notifications**: Set up Slack/Discord alerts for failed verifications
2. **Use Scheduling**: Automate daily verification runs during off-peak hours
3. **Monitor Costs**: Set up alerts for unusual spending patterns
4. **Backup Regularly**: Enable automatic configuration backups
5. **Load Balancing**: Distribute requests across multiple providers

Would you like help implementing any of these?`
}

// generateAnalysisResponse generates analysis responses
func (ai *AIAssistant) generateAnalysisResponse(message string) (string, error) {
	// Get recent verification results
	results, err := ai.db.ListVerificationResults(map[string]interface{}{
		"limit": 10,
	})
	if err != nil {
		return "", fmt.Errorf("failed to fetch results: %w", err)
	}

	if len(results) == 0 {
		return "ðŸ“Š **Analysis Results**\n\nNo recent verification results found. Run some verifications first!", nil
	}

	// Calculate statistics
	total := len(results)
	passed := 0
	failed := 0
	totalScore := 0.0

	for _, result := range results {
		if result.Status == "completed" {
			passed++
			totalScore += result.OverallScore
		} else {
			failed++
		}
	}

	avgScore := totalScore / float64(passed)

	return fmt.Sprintf(`ðŸ“Š **Analysis Results**

**Summary:**
- Total verifications: %d
- Successful: %d (%.1f%%)
- Failed: %d (%.1f%%)
- Average score: %.1f/100

**Performance Insights:**
â€¢ %s success rate indicates %s
â€¢ Average score suggests %s model quality
â€¢ %d failures may need attention

**Recommendations:**
%s`,
		total, passed, float64(passed)/float64(total)*100,
		failed, float64(failed)/float64(total)*100,
		avgScore,
		fmt.Sprintf("%.1f", float64(passed)/float64(total)*100),
		ai.getSuccessRateMessage(float64(passed)/float64(total)),
		ai.getScoreMessage(avgScore),
		failed,
		ai.getRecommendations(avgScore, failed)), nil
}

// generateConfigurationResponse generates configuration help
func (ai *AIAssistant) generateConfigurationResponse(message string) string {
	return `âš™ï¸ **Configuration Assistant**

Let's optimize your LLM Verifier setup:

ðŸ”§ **Quick Wins:**
1. Enable Notifications: Get alerts for failures and anomalies
2. Set Up Scheduling: Automate verification runs
3. Configure Backups: Never lose your settings
4. Add Rate Limiting: Prevent API quota exhaustion

ðŸ“‹ **Step-by-Step Guide:**

1. For Notifications:
   notifications:
     slack:
       enabled: true
       webhook_url: "your-webhook-url"

2. For Scheduling:
   schedules:
     - name: "daily-verification"
       type: "cron"
       expression: "0 2 * * *"  # Daily at 2 AM

3. For Monitoring:
   monitoring:
     enabled: true
     alert_threshold: 95.0

Need help with a specific configuration? Just ask!`
}

// generateGeneralResponse generates a general conversational response
func (ai *AIAssistant) generateGeneralResponse(message string) string {
	responses := []string{
		"That's an interesting question! Let me help you with that.",
		"I understand you're asking about LLM verification. How can I assist?",
		"Great question! Here's what I can tell you:",
		"I'm here to help with all your LLM verification needs.",
		"Let me provide some insights on that topic.",
	}

	// Simple response selection based on message length
	index := len(message) % len(responses)
	return responses[index]
}

// Helper methods
func (ai *AIAssistant) addToContext(userID, message string) {
	if ai.context[userID] == nil {
		ai.context[userID] = make([]string, 0)
	}

	ai.context[userID] = append(ai.context[userID], message)

	// Keep only last 10 messages
	if len(ai.context[userID]) > 10 {
		ai.context[userID] = ai.context[userID][len(ai.context[userID])-10:]
	}
}

func (ai *AIAssistant) getSuccessRateMessage(rate float64) string {
	if rate >= 0.95 {
		return "excellent system reliability"
	} else if rate >= 0.85 {
		return "good overall performance"
	} else if rate >= 0.75 {
		return "acceptable but could be improved"
	}
	return "needs attention"
}

func (ai *AIAssistant) getScoreMessage(score float64) string {
	if score >= 90 {
		return "high-quality"
	} else if score >= 80 {
		return "good"
	} else if score >= 70 {
		return "moderate"
	}
	return "variable"
}

func (ai *AIAssistant) getRecommendations(score float64, failures int) string {
	var recs []string

	if score < 85 {
		recs = append(recs, "â€¢ Consider upgrading to higher-quality models")
	}

	if failures > 0 {
		recs = append(recs, "â€¢ Investigate and resolve verification failures")
	}

	if len(recs) == 0 {
		recs = append(recs, "â€¢ Your system is performing well! Keep monitoring.")
	}

	recs = append(recs, "â€¢ Regular maintenance checks recommended")
	recs = append(recs, "â€¢ Consider enabling advanced analytics for deeper insights")

	return strings.Join(recs, "\n")
}

// registerBuiltInPlugins registers the default plugins
func (ai *AIAssistant) registerBuiltInPlugins() {
	// Simple built-in plugins for demonstration
	sentimentPlugin := &SimpleSentimentPlugin{}
	ai.Plugins.RegisterPlugin(sentimentPlugin)

	codeReviewPlugin := &SimpleCodeReviewPlugin{}
	ai.Plugins.RegisterPlugin(codeReviewPlugin)

	perfPlugin := &SimplePerformancePlugin{}
	ai.Plugins.RegisterPlugin(perfPlugin)
}

// GetPlugins returns the plugin manager
func (ai *AIAssistant) GetPlugins() *PluginManager {
	return ai.Plugins
}

// GetCache returns the cache manager
func (ai *AIAssistant) GetCache() *CacheManager {
	return ai.cache
}

// EnableCache enables caching
func (ai *AIAssistant) EnableCache() {
	ai.cache.Enable()
}

// DisableCache disables caching
func (ai *AIAssistant) DisableCache() {
	ai.cache.Disable()
}

// ClearCache clears all cached responses
func (ai *AIAssistant) ClearCache() error {
	return ai.cache.Clear()
}

// GetCacheStats returns cache statistics
func (ai *AIAssistant) GetCacheStats() map[string]interface{} {
	// Since we don't have direct access to internal stats,
	// return basic information
	return map[string]interface{}{
		"enabled": ai.cache.IsEnabled(),
		"type":    "in_memory",
		"note":    "Detailed statistics not available for in-memory cache",
	}
}

// Simple built-in plugins for demonstration

// SimpleSentimentPlugin provides basic sentiment analysis
type SimpleSentimentPlugin struct{}

// Name returns the plugin name
func (p *SimpleSentimentPlugin) Name() string { return "sentiment_analysis" }

// Version returns the plugin version
func (p *SimpleSentimentPlugin) Version() string { return "1.0.0" }

// Description returns the plugin description
func (p *SimpleSentimentPlugin) Description() string {
	return "Basic sentiment analysis for text processing"
}

// Initialize initializes the plugin
func (p *SimpleSentimentPlugin) Initialize(config map[string]interface{}) error {
	return nil
}

// Execute executes sentiment analysis
func (p *SimpleSentimentPlugin) Execute(ctx context.Context, input interface{}) (interface{}, error) {
	text, ok := input.(string)
	if !ok {
		return nil, fmt.Errorf("input must be a string")
	}

	// Very basic sentiment analysis
	score := 0.5 // neutral
	if strings.Contains(strings.ToLower(text), "good") || strings.Contains(strings.ToLower(text), "great") {
		score = 0.8
	} else if strings.Contains(strings.ToLower(text), "bad") || strings.Contains(strings.ToLower(text), "terrible") {
		score = 0.2
	}

	return map[string]interface{}{
		"text":      text,
		"sentiment": "neutral",
		"score":     score,
	}, nil
}

// Shutdown shuts down the plugin
func (p *SimpleSentimentPlugin) Shutdown() error { return nil }

// GetCapabilities returns plugin capabilities
func (p *SimpleSentimentPlugin) GetCapabilities() []string {
	return []string{"sentiment_analysis", "text_processing"}
}

// SimpleCodeReviewPlugin provides basic code review
type SimpleCodeReviewPlugin struct{}

// Name returns the plugin name
func (p *SimpleCodeReviewPlugin) Name() string { return "code_review" }

// Version returns the plugin version
func (p *SimpleCodeReviewPlugin) Version() string { return "1.0.0" }

// Description returns the plugin description
func (p *SimpleCodeReviewPlugin) Description() string {
	return "Basic code review and quality analysis"
}

// Initialize initializes the plugin
func (p *SimpleCodeReviewPlugin) Initialize(config map[string]interface{}) error {
	return nil
}

// Execute executes code review
func (p *SimpleCodeReviewPlugin) Execute(ctx context.Context, input interface{}) (interface{}, error) {
	code, ok := input.(string)
	if !ok {
		return nil, fmt.Errorf("input must be a string")
	}

	// Very basic code analysis
	issues := []string{}
	if strings.Contains(code, "TODO") {
		issues = append(issues, "Contains TODO comments")
	}
	if len(code) > 1000 {
		issues = append(issues, "Long function/method")
	}

	return map[string]interface{}{
		"code":          code,
		"issues":        issues,
		"quality_score": 100 - len(issues)*10,
		"language":      "detected",
	}, nil
}

// Shutdown shuts down the plugin
func (p *SimpleCodeReviewPlugin) Shutdown() error { return nil }

// GetCapabilities returns plugin capabilities
func (p *SimpleCodeReviewPlugin) GetCapabilities() []string {
	return []string{"code_review", "quality_analysis"}
}

// SimplePerformancePlugin provides basic performance analysis
type SimplePerformancePlugin struct{}

// Name returns the plugin name
func (p *SimplePerformancePlugin) Name() string { return "performance_analysis" }

// Version returns the plugin version
func (p *SimplePerformancePlugin) Version() string { return "1.0.0" }

// Description returns the plugin description
func (p *SimplePerformancePlugin) Description() string {
	return "Basic performance analysis and monitoring"
}

// Initialize initializes the plugin
func (p *SimplePerformancePlugin) Initialize(config map[string]interface{}) error {
	return nil
}

// Execute executes performance analysis
func (p *SimplePerformancePlugin) Execute(ctx context.Context, input interface{}) (interface{}, error) {
	data, ok := input.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("input must be a map")
	}

	// Very basic performance metrics
	responseTime := 100.0 // ms
	if rt, ok := data["response_time"].(float64); ok {
		responseTime = rt
	}

	performance := "good"
	if responseTime > 500 {
		performance = "poor"
	} else if responseTime > 200 {
		performance = "fair"
	}

	return map[string]interface{}{
		"response_time_ms": responseTime,
		"performance":      performance,
		"recommendation":   "Optimize if response time > 200ms",
	}, nil
}

// Shutdown shuts down the plugin
func (p *SimplePerformancePlugin) Shutdown() error { return nil }

// GetCapabilities returns plugin capabilities
func (p *SimplePerformancePlugin) GetCapabilities() []string {
	return []string{"performance_analysis", "monitoring"}
}
